{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Boston House Price\n",
    "\n",
    "----\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Introduction to the Dataset & Features](#introduction)\n",
    "2. [Loading the Dataset](#load)\n",
    "3. [Understand the Dataset](#understand)\n",
    "4. [Understand the Datatypes](#dtypes)\n",
    "5. [Visualizing the DV and IVs](#vis1)\n",
    "6. [Calculate SST](#sst)\n",
    "7. [SST vs SSR vs SSE](#SSTSSRSSE)\n",
    "8. [Univariate Linear Regression](#best_feat)\n",
    "9. [Summary of Univariate LinReg](#summ1)\n",
    "10.[Multiple Linear Regression](#mlreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Features <a name=\"introduction\"></a>\n",
    "\n",
    "A project for learning few optimal solutions to predict housing prices using Boston Housing Prices, uses supervised learning with multilinear regression (not to be confused with multivariate regression). The provided data by the SKLearn (Boston House) is rather clean and fit for purpose of learning. In practices, there might be a big chance that the data is less useful and preprocessing is rather needed. This project shall not focus on preprocessing such as, interpolate missing values, etc.\n",
    "\n",
    "There are 13 features are gonna be used according to the database which you can find further details from [here](https://www.kaggle.com/c/boston-housing) and how to load it with SKLearn from [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston).\n",
    "\n",
    "(As quoted from the DESCR attributes of the dataset) All attribute Information in order are:\n",
    "* CRIM     per capita crime rate by town\n",
    "* ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS    proportion of non-retail business acres per town\n",
    "* CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX      nitric oxides concentration (parts per 10 million)\n",
    "* RM       average number of rooms per dwelling\n",
    "* AGE      proportion of owner-occupied units built prior to 1940\n",
    "* DIS      weighted distances to five Boston employment centres\n",
    "* RAD      index of accessibility to radial highways\n",
    "* TAX      full-value property-tax rate per USD 10,000\n",
    "* PTRATIO  pupil-teacher ratio by town\n",
    "* B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT    Lower status of the population\n",
    "* MEDV     Median value of owner-occupied homes in USD 1000's\n",
    "\n",
    "In the project, each I.V. features versus D.V. correlations will be evaluated to see how relevant they are to build the best predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset <a name=\"load\"></a>\n",
    "\n",
    "The dataset is available through [Kaggle](www.kaggle.com) but since we're gonna use [SKLearn](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston) anyway, so I picked the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# matplotlib inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the Dataset <a name=\"understand\"></a>\n",
    "\n",
    "Make sure you understand what informations are provided by the dataset. Read through all the definitions of the features and the target (outcome). Since I will use Pandas as dataframe for most of the data, we need to build the dataframe as the first step. Make sure to visualize it in a good way in such you get the big picture of how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>HOPRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  HOPRICE  \n",
       "0     15.3  396.90   4.98     24.0  \n",
       "1     17.8  396.90   9.14     21.6  \n",
       "2     17.8  392.83   4.03     34.7  \n",
       "3     18.7  394.63   2.94     33.4  \n",
       "4     18.7  396.90   5.33     36.2  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "boston = load_boston()\n",
    "\n",
    "# build the dataframe for all I.Vs (boston['data']) or boston.data\n",
    "df_boston_data = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
    "\n",
    "# concatenate the D.V (boston['target']) and name id 'HOPRICE'\n",
    "df_boston_data['HOPRICE'] = pd.Series(boston['target'])\n",
    "df_boston_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see **HOPRICE** is part of the dataset and we're good to go. Remember, in the real-world data, we mostly will encounter significantly more random data and you would likely need a lot of preprocessing before you see the same structure as shown above.\n",
    "\n",
    "### Understand the Datatypes <a name=\"dtypes\"></a>\n",
    "\n",
    "Pandas is capable of easily giving us informations on each features datatype. DataFrame will infer automatically the datatypes and what we're gonna do is to see their types. What we want from this step is to differentiate their types according to the proper statistic types as explained in [here](https://en.wikipedia.org/wiki/Statistical_data_type). As we see below, everything are neither Categorical or Binary, but instead, all are continuous values and as expected the **HOPRICE** itself is not a Categorical thus we're not gonna do any Logistic Regression to create our best fit as Linear Regression (hopefully) will sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       float64\n",
       "ZN         float64\n",
       "INDUS      float64\n",
       "CHAS       float64\n",
       "NOX        float64\n",
       "RM         float64\n",
       "AGE        float64\n",
       "DIS        float64\n",
       "RAD        float64\n",
       "TAX        float64\n",
       "PTRATIO    float64\n",
       "B          float64\n",
       "LSTAT      float64\n",
       "HOPRICE    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_boston_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Start Visualizing D.V without any inputting I.Vs. <a name=\"vis1\"></a>\n",
    "\n",
    "We ought to visualize the relation between I.Vs (all features) to the D.V to see how significant their impacts to the D.V. That can be done by manually visualizing them and see a pattern that can be fit by our model, which in this case Linear Regression best fit model. However, let's visualize the **HOPRICE** alone and see how we will see the best fit naturally using **Mean** of the data.\n",
    "\n",
    "To give a better understanding, an equation for linear line is given by:\n",
    "\n",
    "$$ f(x) = mx + b + e$$\n",
    "\n",
    "Given the linear equation, *m* is the slope or gradient, or later will be addressed as *coefficient* and *b* is the y-interceptor and residual error as *e*. If there's no valid *x* variables and no residual error, then best fit line would be:\n",
    "\n",
    "$$ f(0) = b $$\n",
    "\n",
    "This is saying that the most basic way to estimate house price without an independent variable is to start with the *b* which in most cases interpreted as the *mean* values. Here let's calculate the mean value of **HOPRICE** and plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.532806324110698"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean\n",
    "m_hoprice = df_boston_data['HOPRICE'].mean()\n",
    "m_hoprice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we're gonna do a lot of visualizations, picking best plotting API is important. Here, best option is to use [Seaborn](https://seaborn.pydata.org/) for correlation plot and but for interactive plot, one of the alternative is [Plotly](https://plot.ly/python/). I will try to use both and later, will use Seaborn more often than others since it is specifically purposed for statistical data visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~yanuart.adityan/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"PLOTLY.js\"\"\"\n",
    "\n",
    "# scatter plots for housing prices\n",
    "trace_hoprice = Scatter(\n",
    "    x=df_boston_data.index,\n",
    "    y=df_boston_data['HOPRICE'],\n",
    "    name='HousingPrice',\n",
    "    mode='markers')\n",
    "\n",
    "# line for b-val/mean\n",
    "trace_mhoprice = Scatter(\n",
    "    x=[0, df_boston_data.shape[0]],\n",
    "    y=[m_hoprice, m_hoprice],\n",
    "    name='Mean Values %.2f' % m_hoprice,\n",
    "    mode='lines',\n",
    "    line=dict(\n",
    "        width=3))\n",
    "\n",
    "# create the data\n",
    "data = [trace_hoprice, trace_mhoprice]\n",
    "\n",
    "# layouting and plot\n",
    "layout = Layout(\n",
    "    title='Housing Prices vs Mean Values as Best Fit',\n",
    "    xaxis=dict(title='house index'),\n",
    "    yaxis=dict(title='house price $1000'),\n",
    "    showlegend=True)\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='housing-price-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SST (Total of Squared Error) using Mean Best Fit <a name=\"SST\"></a>\n",
    "\n",
    "To evaluate the values of the current best fit here, we need to calculate how each elements of the dataset variates from our current best fit line using **Mean** best fit. To do this, we just need to find the $\\sum (y_i - \\overline y)^2$ and sum all of them with $\\overline y$ is the **mean** or *b*. \n",
    "\n",
    "Code wise, to do this, let's create another dataframe which comprised of *y*, residual $(y_i - \\overline y)$, and squared of the residual $(y_i - \\overline y)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOPRICE</th>\n",
       "      <th>RES</th>\n",
       "      <th>RES2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.467194</td>\n",
       "      <td>2.152657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "      <td>-0.932806</td>\n",
       "      <td>0.870128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "      <td>12.167194</td>\n",
       "      <td>148.040602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "      <td>10.867194</td>\n",
       "      <td>118.095898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "      <td>13.667194</td>\n",
       "      <td>186.792183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HOPRICE        RES        RES2\n",
       "0     24.0   1.467194    2.152657\n",
       "1     21.6  -0.932806    0.870128\n",
       "2     34.7  12.167194  148.040602\n",
       "3     33.4  10.867194  118.095898\n",
       "4     36.2  13.667194  186.792183"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup the first column - HOPRICE\n",
    "df_calc_sst = pd.DataFrame(data=df_boston_data['HOPRICE'])\n",
    "# df_calc_sse.loc['MEAN'] = df_calc_sse.mean()\n",
    "\n",
    "# add the residual - RES\n",
    "df_calc_sst['RES'] = df_calc_sst['HOPRICE'] - df_calc_sst['HOPRICE'].mean()\n",
    "\n",
    "# add the squared residual -RES2\n",
    "df_calc_sst['RES2'] = df_calc_sst['RES'] ** 2\n",
    "\n",
    "# check\n",
    "df_calc_sst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42716.29541501979"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sse = np.sum(df_calc_sst['RES2'])\n",
    "val_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST vs SSR vs SSE <a name=\"SSTSSRSSE\"></a>\n",
    "\n",
    "Now it is known that the sum of squared residual is **42716.2954** which is now addressed as SST. Later, this SST will be used to be the basic value for the performance benchmarking of our linear best fit model following this simple law:\n",
    "\n",
    "$$ SST =  SSR + SSE $$\n",
    "\n",
    "which *SST* is the total of squared error from using mean fit, and *SSE* will be the new sum of squared error from using our linear regression model. *SSR* will be the value to look up to since it represents how **much our linear regression eliminates error from the dataset**. \n",
    "\n",
    "Now let's do one of the most interesting in preprocessing the dataset, finding the best features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Linear Regression <a name=\"best_feat\"></a>\n",
    "\n",
    "Finding the best features, by meaning, is to find the best correlated features which contributes to the dependent variable D.V which in this case **HOPRICE**. Obvious selection, would be to guess the best feature and for the sake of it, let's pick feature **RM** number of rooms in the house. Let's start by visualizing X-Y scattered plot with **RM** for the X-axis and **HOPRICE** for the Y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~yanuart.adityan/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_rm = Scatter(\n",
    "    x=df_boston_data['RM'],\n",
    "    y=df_boston_data['HOPRICE'],\n",
    "    name='room vs house price',\n",
    "    mode='markers')\n",
    "\n",
    "# layouting and plot\n",
    "layout = Layout(\n",
    "    title='Number of Rooms vs Housing Prices',\n",
    "    xaxis=dict(title='number of rooms'),\n",
    "    yaxis=dict(title='house price $1000'),\n",
    "    showlegend=True)\n",
    "\n",
    "fig = Figure(data=[trace_rm], layout=layout)\n",
    "py.iplot(fig, filename='housing-price-vs-rooms-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as you can see there is a trend between growing number of rooms to the increaing price of the house. The trend here can be found by Linear Regression which gives us the best coefficient *m* and *intercept* which satisfy equation in [here](#vis1) for given independent feature. \n",
    "\n",
    "To do this, let's create an object of LinearRegression class from Scikit-Learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an object for Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare the feature data **RM** and target **HOPRICE**, and split the training and test ratio by an arbitrary number of 0.8 which means 80% of all sample data will be for training to get best fit line and 20% for the test. One can say that pick the ratio either 80/20 or 70/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have training data with (404,) number or rows\n",
      "We have test data with (102,) number of rows\n"
     ]
    }
   ],
   "source": [
    "# split the training and test\n",
    "from sklearn import cross_validation\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(df_boston_data['RM'].values, \n",
    "                                                                     df_boston_data['HOPRICE'].values,\n",
    "                                                                     train_size=0.8)\n",
    "\n",
    "print('We have training data with {} number or rows'.format(x_train.shape))\n",
    "print('We have test data with {} number of rows'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the splitted training and test data, the *fit* method only accepts 2D array input, thus we need to reshape above set to (*N_samples*, 1) using *reshape*. Then, we can use the training set *x_train* and *y_train* as input to the fitting method from the *lreg* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: 8.9067\n",
      "y-intercept: -33.4557\n"
     ]
    }
   ],
   "source": [
    "# reshape\n",
    "x_train = np.reshape(x_train, (len(x_train), 1))\n",
    "y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 1))\n",
    "y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "# fit\n",
    "lreg.fit(x_train, y_train)\n",
    "\n",
    "print('coefficients: %.4f' % lreg.coef_)\n",
    "print('y-intercept: %.4f' % lreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with known coefficient *m* and y-intercept *b*, the equation shall be:\n",
    "\n",
    "$$ \\hat y = mx + b \\\\\n",
    "\\hat y = 8.7567x - 32.6496 \n",
    "$$\n",
    "\n",
    "The best fit now can be visualize using Seaborn *regplot* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'RM vs HOPRICE with f(x)=8.9067x + (-33.4557) best fit')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT72qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ea2d05eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.regplot(x='RM', y='HOPRICE', data=df_boston_data)\n",
    "ax.set_title('RM vs HOPRICE with f(x)=%.4fx + (%.4f) best fit' % (lreg.coef_, lreg.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the SSE of the Best Fit (Training vs Test)\n",
    "\n",
    "Now that we have the model, we have to predict using both training and test sets. To evaluate the accuracy of the model, we refer back to the SSE of both. Let's predict using both sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XTRAIN</th>\n",
       "      <th>YTRAIN</th>\n",
       "      <th>PREDTRAIN</th>\n",
       "      <th>RES</th>\n",
       "      <th>RES2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.854</td>\n",
       "      <td>10.8</td>\n",
       "      <td>18.683931</td>\n",
       "      <td>-3.731168</td>\n",
       "      <td>13.921611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.860</td>\n",
       "      <td>29.9</td>\n",
       "      <td>27.644042</td>\n",
       "      <td>5.228943</td>\n",
       "      <td>27.341848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.140</td>\n",
       "      <td>20.8</td>\n",
       "      <td>21.231239</td>\n",
       "      <td>-1.183860</td>\n",
       "      <td>1.401524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.337</td>\n",
       "      <td>41.7</td>\n",
       "      <td>40.799195</td>\n",
       "      <td>18.384096</td>\n",
       "      <td>337.974990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.072</td>\n",
       "      <td>14.5</td>\n",
       "      <td>20.625586</td>\n",
       "      <td>-1.789513</td>\n",
       "      <td>3.202358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   XTRAIN  YTRAIN  PREDTRAIN        RES        RES2\n",
       "0   5.854    10.8  18.683931  -3.731168   13.921611\n",
       "1   6.860    29.9  27.644042   5.228943   27.341848\n",
       "2   6.140    20.8  21.231239  -1.183860    1.401524\n",
       "3   8.337    41.7  40.799195  18.384096  337.974990\n",
       "4   6.072    14.5  20.625586  -1.789513    3.202358"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict train\n",
    "pred_train = lreg.predict(x_train)\n",
    "\n",
    "# create dataframe to show\n",
    "df_result = pd.DataFrame(data=np.hstack((x_train, y_train, pred_train)), columns=['XTRAIN', 'YTRAIN', 'PREDTRAIN'])\n",
    "\n",
    "# find error (RES) and RES**2\n",
    "df_result['RES'] = df_result['PREDTRAIN'] - df_result['YTRAIN'].mean()\n",
    "\n",
    "# add the squared residual -RES2\n",
    "df_result['RES2'] = df_result['RES'] ** 2\n",
    "\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XTEST</th>\n",
       "      <th>YTEST</th>\n",
       "      <th>PREDTEST</th>\n",
       "      <th>RES</th>\n",
       "      <th>RES2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.747</td>\n",
       "      <td>8.5</td>\n",
       "      <td>17.730918</td>\n",
       "      <td>-5.268102</td>\n",
       "      <td>27.752898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.241</td>\n",
       "      <td>32.7</td>\n",
       "      <td>31.037484</td>\n",
       "      <td>8.038464</td>\n",
       "      <td>64.616908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.807</td>\n",
       "      <td>22.4</td>\n",
       "      <td>18.265318</td>\n",
       "      <td>-4.733702</td>\n",
       "      <td>22.407932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.016</td>\n",
       "      <td>50.0</td>\n",
       "      <td>29.033483</td>\n",
       "      <td>6.034463</td>\n",
       "      <td>36.414748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.895</td>\n",
       "      <td>18.5</td>\n",
       "      <td>19.049105</td>\n",
       "      <td>-3.949915</td>\n",
       "      <td>15.601826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   XTEST  YTEST   PREDTEST       RES       RES2\n",
       "0  5.747    8.5  17.730918 -5.268102  27.752898\n",
       "1  7.241   32.7  31.037484  8.038464  64.616908\n",
       "2  5.807   22.4  18.265318 -4.733702  22.407932\n",
       "3  7.016   50.0  29.033483  6.034463  36.414748\n",
       "4  5.895   18.5  19.049105 -3.949915  15.601826"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict test\n",
    "pred_test = lreg.predict(x_test)\n",
    "\n",
    "# create dataframe to show\n",
    "df_result_test = pd.DataFrame(data=np.hstack((x_test, y_test, pred_test)), columns=['XTEST', 'YTEST', 'PREDTEST'])\n",
    "\n",
    "# find error (RES) and RES**2\n",
    "df_result_test['RES'] = df_result_test['PREDTEST'] - df_result_test['YTEST'].mean()\n",
    "\n",
    "# add the squared residual -RES2\n",
    "df_result_test['RES2'] = df_result_test['RES'] ** 2\n",
    "\n",
    "df_result_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the SSE from the both training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Error (SSE) for training dataset is 16150.7868\n",
      "Sum of Squared Error (SSE) for test dataset is 3604.8033\n"
     ]
    }
   ],
   "source": [
    "pred_train_sse = np.sum(df_result['RES2'])\n",
    "pred_test_sse = np.sum(df_result_test['RES2'])\n",
    "\n",
    "print('Sum of Squared Error (SSE) for training dataset is %.4f' % pred_train_sse)\n",
    "print('Sum of Squared Error (SSE) for test dataset is %.4f' % pred_test_sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see much more improved here with the current model compared to **SST** of 42716.2954 shown [here](#SSTSSRSSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Univariate Regression Model <a name=\"summ1\"></a>\n",
    "\n",
    "Now since we have SSE of our linear regression model from both training and test dataset, now we want to summarize of how the **SSR** shall be. Using, training dataset, the **SSR** shall be $SST-SSE$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------Recap---------------------------------------------\n",
      "The total of squared errors (SST) is: 42716.2954\n",
      "The SSR for training data (SSR_Train) is: 26565.5086\n",
      "Given training data, the model is capable to remove 62.1906 percent of error from SST\n",
      "The SSR for test data (SSR_Train) is: 39111.4921\n",
      "Given test data, the model is capable to remove 91.5611 percent of error from SST\n"
     ]
    }
   ],
   "source": [
    "SSR_training = val_sse - pred_train_sse\n",
    "SSR_test = val_sse - pred_test_sse\n",
    "\n",
    "SSR_training_percent = SSR_training/val_sse * 100.0\n",
    "SSR_test_percent = SSR_test/val_sse * 100.0\n",
    "\n",
    "print('---------------------------------------------Recap---------------------------------------------')\n",
    "print('The total of squared errors (SST) is: %.4f' % val_sse)\n",
    "print('The SSR for training data (SSR_Train) is: %.4f' % SSR_training)\n",
    "print('Given training data, the model is capable to remove %.4f percent of error from SST' % SSR_training_percent)\n",
    "print('The SSR for test data (SSR_Train) is: %.4f' % SSR_test)\n",
    "print('Given test data, the model is capable to remove %.4f percent of error from SST' % SSR_test_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Well well well, it seems like we have managed to reduce the squared residual errors by almost 90% on the test data just by using number of rooms as a single independent variable. What if we add more? Enter *Multiple Linear Regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression <a name=\"mlreg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
